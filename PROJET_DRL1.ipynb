{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4l1VL26bUWF",
        "outputId": "af2b646c-724e-443d-f26f-5176a53822f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "  Downloading roboschool-1.0.48-cp37-cp37m-manylinux1_x86_64.whl (44.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "  Downloading gym-0.15.4.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 41.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "  Downloading pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-py3-none-any.whl size=1648485 sha256=d4cc020ca0930e39d64060671ef8ca6fb2a0a7752b443aef77234bef4aa4d73a\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/97/51/3adbfe67f40bce89b8eba2d3b8f42ec1c9f9c1e6305a73510d\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Attempting uninstall: pyglet\n",
            "    Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 13.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (90.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 90.8 MB 264 bytes/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install roboschool==1.0.48 gym==0.15.4\n",
        "\n",
        "!pip install box2d-py # 2d rigid body physics engine\n",
        "\n",
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOG4FJMoxt2j"
      },
      "source": [
        "**Import libraries and set device to gpu**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsGHSa40roi6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "import numpy as np\n",
        "import gym\n",
        "import roboschool\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJCLSXEMOx2i"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTe0hVSjsEuh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "    def store_memory(self, state, action, action_logprob):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(action_logprob)\n",
        "    \n",
        "    \n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.actions=[]\n",
        "        self.states=[]\n",
        "        self.probs=[]\n",
        "        self.rewards=[]\n",
        "        self.dones=[]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.action_dim = action_dim\n",
        "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        self.actor = nn.Sequential(\n",
        "                        nn.Linear(state_dim,64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64 ,64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, action_dim),\n",
        "                        nn.Tanh()   #if discrete action space = sftmax\n",
        "                    )\n",
        "\n",
        "\n",
        "        \n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "    def act(self, state):\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        return action.detach(), action_logprob.detach()\n",
        "    def criticize(self, state, action):\n",
        "        action_mean = self.actor(state)\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(device)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action_probs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_probs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std_init=0.1):\n",
        "\n",
        "\n",
        "        self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = PPOMemory()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim,  action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim,  action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "    def select_action(self, state):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "        self.buffer.store_memory(state,action,action_logprob)\n",
        "\n",
        "\n",
        "        return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "            \n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        # rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_probs = torch.squeeze(torch.stack(self.buffer.probs, dim=0)).detach().to(device)\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            probs, state_values, dist_entropy = self.policy.criticize(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(probs - old_probs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()   \n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear_memory()\n",
        "    \n",
        "    \n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "76WiiB8-Bqlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**environnement & algorithm Hyperparemeters** "
      ],
      "metadata": {
        "id": "ISWqLhJYBubl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"RoboschoolAtlasForwardWalk-v1\"\n",
        "max_ep_len = 1000                   # max timesteps in one episode\n",
        "max_training_timesteps = int(1000000)   # break training loop if timeteps > max_training_timesteps\n",
        "print_freq = max_ep_len * 10        # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2           # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = 10000             # save model frequency (in num timesteps)\n",
        "action_std = 0.1                    # starting std for action distribution (Multivariate Normal)\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 80               # update policy for K epochs in one PPO update The idea in PPO is that you want to reuse the batch many times to update the current policy.This means you repeat your training' k. epoch amount of times for the same batch of trajectories.\n",
        "eps_clip = 0.2          # clip parameter for PPO 2 \n",
        "gamma = 0.99            # discount factor\n",
        "lr_actor = 0.001       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n"
      ],
      "metadata": {
        "id": "4wX2e1QIBuDM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xCuXGQ5lJZw",
        "outputId": "8d42694d-667b-4fa1-e4e4-24793e6f5259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logging at : Logs/PPO_RoboschoolAtlasForwardWalk-v1_log_1.csv\n",
            "save checkpoint path : Checkpoints/RoboschoolAtlasForwardWalk-v1/checkpoint_0_0.pth\n",
            "state space dimension :  70\n",
            "action space dimension :  30\n",
            "Episode : 353 \t\t Timestep : 10000 \t\t Average Reward : 43.49\n",
            "model saved\n",
            "Episode : 700 \t\t Timestep : 20000 \t\t Average Reward : 43.44\n",
            "model saved\n",
            "Episode : 1055 \t\t Timestep : 30000 \t\t Average Reward : 44.98\n",
            "model saved\n",
            "Episode : 1407 \t\t Timestep : 40000 \t\t Average Reward : 48.57\n",
            "model saved\n",
            "Episode : 1758 \t\t Timestep : 50000 \t\t Average Reward : 48.63\n",
            "model saved\n",
            "Episode : 2110 \t\t Timestep : 60000 \t\t Average Reward : 51.14\n",
            "model saved\n",
            "Episode : 2462 \t\t Timestep : 70000 \t\t Average Reward : 52.28\n",
            "model saved\n",
            "Episode : 2814 \t\t Timestep : 80000 \t\t Average Reward : 53.01\n",
            "model saved\n",
            "Episode : 3168 \t\t Timestep : 90000 \t\t Average Reward : 53.18\n",
            "model saved\n",
            "Episode : 3512 \t\t Timestep : 100000 \t\t Average Reward : 56.52\n",
            "model saved\n",
            "Episode : 3860 \t\t Timestep : 110000 \t\t Average Reward : 57.09\n",
            "model saved\n",
            "Episode : 4204 \t\t Timestep : 120000 \t\t Average Reward : 58.09\n",
            "model saved\n",
            "Episode : 4546 \t\t Timestep : 130000 \t\t Average Reward : 59.65\n",
            "model saved\n",
            "Episode : 4890 \t\t Timestep : 140000 \t\t Average Reward : 60.32\n",
            "model saved\n",
            "Episode : 5231 \t\t Timestep : 150000 \t\t Average Reward : 61.37\n",
            "model saved\n",
            "Episode : 5576 \t\t Timestep : 160000 \t\t Average Reward : 61.84\n",
            "model saved\n",
            "Episode : 5925 \t\t Timestep : 170000 \t\t Average Reward : 62.23\n",
            "model saved\n",
            "Episode : 6266 \t\t Timestep : 180000 \t\t Average Reward : 64.93\n",
            "model saved\n",
            "Episode : 6605 \t\t Timestep : 190000 \t\t Average Reward : 65.76\n",
            "model saved\n",
            "Episode : 6943 \t\t Timestep : 200000 \t\t Average Reward : 67.75\n",
            "model saved\n",
            "Episode : 7276 \t\t Timestep : 210000 \t\t Average Reward : 67.82\n",
            "model saved\n",
            "Episode : 7612 \t\t Timestep : 220000 \t\t Average Reward : 68.52\n",
            "model saved\n",
            "Episode : 7939 \t\t Timestep : 230000 \t\t Average Reward : 69.68\n",
            "model saved\n",
            "Episode : 8266 \t\t Timestep : 240000 \t\t Average Reward : 71.04\n",
            "model saved\n",
            "Episode : 8600 \t\t Timestep : 250000 \t\t Average Reward : 70.37\n",
            "model saved\n",
            "Episode : 8931 \t\t Timestep : 260000 \t\t Average Reward : 71.13\n",
            "model saved\n",
            "Episode : 9262 \t\t Timestep : 270000 \t\t Average Reward : 72.34\n",
            "model saved\n",
            "Episode : 9589 \t\t Timestep : 280000 \t\t Average Reward : 72.39\n",
            "model saved\n",
            "Episode : 9916 \t\t Timestep : 290000 \t\t Average Reward : 72.3\n",
            "model saved\n",
            "Episode : 10244 \t\t Timestep : 300000 \t\t Average Reward : 72.7\n",
            "model saved\n",
            "Episode : 10569 \t\t Timestep : 310000 \t\t Average Reward : 74.91\n",
            "model saved\n",
            "Episode : 10900 \t\t Timestep : 320000 \t\t Average Reward : 73.0\n",
            "model saved\n",
            "Episode : 11227 \t\t Timestep : 330000 \t\t Average Reward : 76.61\n",
            "model saved\n",
            "Episode : 11553 \t\t Timestep : 340000 \t\t Average Reward : 77.57\n",
            "model saved\n",
            "Episode : 11875 \t\t Timestep : 350000 \t\t Average Reward : 77.52\n",
            "model saved\n",
            "Episode : 12195 \t\t Timestep : 360000 \t\t Average Reward : 79.48\n",
            "model saved\n",
            "Episode : 12517 \t\t Timestep : 370000 \t\t Average Reward : 79.4\n",
            "model saved\n",
            "Episode : 12831 \t\t Timestep : 380000 \t\t Average Reward : 81.32\n",
            "model saved\n",
            "Episode : 13151 \t\t Timestep : 390000 \t\t Average Reward : 79.36\n",
            "model saved\n",
            "Episode : 13472 \t\t Timestep : 400000 \t\t Average Reward : 79.61\n",
            "model saved\n",
            "Episode : 13788 \t\t Timestep : 410000 \t\t Average Reward : 81.94\n",
            "model saved\n",
            "Episode : 14103 \t\t Timestep : 420000 \t\t Average Reward : 82.91\n",
            "model saved\n",
            "Episode : 14419 \t\t Timestep : 430000 \t\t Average Reward : 81.79\n",
            "model saved\n",
            "Episode : 14732 \t\t Timestep : 440000 \t\t Average Reward : 83.37\n",
            "model saved\n",
            "Episode : 15040 \t\t Timestep : 450000 \t\t Average Reward : 84.81\n",
            "model saved\n",
            "Episode : 15351 \t\t Timestep : 460000 \t\t Average Reward : 84.55\n",
            "model saved\n",
            "Episode : 15657 \t\t Timestep : 470000 \t\t Average Reward : 85.8\n",
            "model saved\n",
            "Episode : 15964 \t\t Timestep : 480000 \t\t Average Reward : 85.21\n",
            "model saved\n",
            "Episode : 16268 \t\t Timestep : 490000 \t\t Average Reward : 87.1\n",
            "model saved\n",
            "Episode : 16573 \t\t Timestep : 500000 \t\t Average Reward : 86.4\n",
            "model saved\n",
            "Episode : 16881 \t\t Timestep : 510000 \t\t Average Reward : 85.45\n",
            "model saved\n",
            "Episode : 17188 \t\t Timestep : 520000 \t\t Average Reward : 86.66\n",
            "model saved\n",
            "Episode : 17496 \t\t Timestep : 530000 \t\t Average Reward : 87.11\n",
            "model saved\n",
            "Episode : 17798 \t\t Timestep : 540000 \t\t Average Reward : 88.86\n",
            "model saved\n",
            "Episode : 18095 \t\t Timestep : 550000 \t\t Average Reward : 89.21\n",
            "model saved\n",
            "Episode : 18395 \t\t Timestep : 560000 \t\t Average Reward : 88.71\n",
            "model saved\n",
            "Episode : 18693 \t\t Timestep : 570000 \t\t Average Reward : 89.98\n",
            "model saved\n",
            "Episode : 18989 \t\t Timestep : 580000 \t\t Average Reward : 90.19\n",
            "model saved\n",
            "Episode : 19284 \t\t Timestep : 590000 \t\t Average Reward : 90.7\n",
            "model saved\n",
            "Episode : 19573 \t\t Timestep : 600000 \t\t Average Reward : 92.19\n",
            "model saved\n",
            "Episode : 19857 \t\t Timestep : 610000 \t\t Average Reward : 93.41\n",
            "model saved\n",
            "Episode : 20143 \t\t Timestep : 620000 \t\t Average Reward : 92.24\n",
            "model saved\n",
            "Episode : 20428 \t\t Timestep : 630000 \t\t Average Reward : 92.72\n",
            "model saved\n",
            "Episode : 20713 \t\t Timestep : 640000 \t\t Average Reward : 93.38\n",
            "model saved\n",
            "Episode : 20992 \t\t Timestep : 650000 \t\t Average Reward : 95.74\n",
            "model saved\n",
            "Episode : 21270 \t\t Timestep : 660000 \t\t Average Reward : 95.34\n",
            "model saved\n",
            "Episode : 21550 \t\t Timestep : 670000 \t\t Average Reward : 96.09\n",
            "model saved\n",
            "Episode : 21832 \t\t Timestep : 680000 \t\t Average Reward : 95.82\n",
            "model saved\n",
            "Episode : 22115 \t\t Timestep : 690000 \t\t Average Reward : 95.39\n",
            "model saved\n",
            "Episode : 22400 \t\t Timestep : 700000 \t\t Average Reward : 94.31\n",
            "model saved\n",
            "Episode : 22682 \t\t Timestep : 710000 \t\t Average Reward : 95.58\n",
            "model saved\n",
            "Episode : 22958 \t\t Timestep : 720000 \t\t Average Reward : 98.11\n",
            "model saved\n",
            "Episode : 23239 \t\t Timestep : 730000 \t\t Average Reward : 96.48\n",
            "model saved\n",
            "Episode : 23516 \t\t Timestep : 740000 \t\t Average Reward : 97.81\n",
            "model saved\n",
            "Episode : 23795 \t\t Timestep : 750000 \t\t Average Reward : 97.27\n",
            "model saved\n",
            "Episode : 24076 \t\t Timestep : 760000 \t\t Average Reward : 97.12\n",
            "model saved\n",
            "Episode : 24352 \t\t Timestep : 770000 \t\t Average Reward : 98.28\n",
            "model saved\n",
            "Episode : 24627 \t\t Timestep : 780000 \t\t Average Reward : 98.23\n",
            "model saved\n",
            "Episode : 24898 \t\t Timestep : 790000 \t\t Average Reward : 99.69\n",
            "model saved\n",
            "Episode : 25167 \t\t Timestep : 800000 \t\t Average Reward : 100.49\n",
            "model saved\n",
            "Episode : 25436 \t\t Timestep : 810000 \t\t Average Reward : 100.57\n",
            "model saved\n",
            "Episode : 25700 \t\t Timestep : 820000 \t\t Average Reward : 102.9\n",
            "model saved\n",
            "Episode : 25964 \t\t Timestep : 830000 \t\t Average Reward : 102.01\n",
            "model saved\n",
            "Episode : 26231 \t\t Timestep : 840000 \t\t Average Reward : 100.68\n",
            "model saved\n",
            "Episode : 26493 \t\t Timestep : 850000 \t\t Average Reward : 102.07\n",
            "model saved\n",
            "Episode : 26753 \t\t Timestep : 860000 \t\t Average Reward : 103.51\n",
            "model saved\n",
            "Episode : 27011 \t\t Timestep : 870000 \t\t Average Reward : 104.33\n",
            "model saved\n",
            "Episode : 27266 \t\t Timestep : 880000 \t\t Average Reward : 106.05\n",
            "model saved\n",
            "Episode : 27519 \t\t Timestep : 890000 \t\t Average Reward : 105.2\n",
            "model saved\n",
            "Episode : 27778 \t\t Timestep : 900000 \t\t Average Reward : 103.62\n",
            "model saved\n",
            "Episode : 28035 \t\t Timestep : 910000 \t\t Average Reward : 106.02\n",
            "model saved\n",
            "Episode : 28292 \t\t Timestep : 920000 \t\t Average Reward : 106.02\n",
            "model saved\n",
            "Episode : 28541 \t\t Timestep : 930000 \t\t Average Reward : 108.79\n",
            "model saved\n",
            "Episode : 28796 \t\t Timestep : 940000 \t\t Average Reward : 107.09\n",
            "model saved\n",
            "Episode : 29052 \t\t Timestep : 950000 \t\t Average Reward : 106.15\n",
            "model saved\n",
            "Episode : 29300 \t\t Timestep : 960000 \t\t Average Reward : 110.57\n",
            "model saved\n",
            "Episode : 29546 \t\t Timestep : 970000 \t\t Average Reward : 110.22\n",
            "model saved\n",
            "Episode : 29796 \t\t Timestep : 980000 \t\t Average Reward : 110.1\n",
            "model saved\n",
            "Episode : 30046 \t\t Timestep : 990000 \t\t Average Reward : 109.08\n",
            "model saved\n",
            "Episode : 30294 \t\t Timestep : 1000000 \t\t Average Reward : 110.29\n",
            "model saved\n"
          ]
        }
      ],
      "source": [
        "# Training \n",
        "env = gym.make(env_name)\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "# action space dimension\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"Logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)#kaddeh mawjoud men wahed deja\n",
        "\n",
        "\n",
        "\n",
        "#### create new log file for each run \n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"Checkpoints\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"checkpoint_{}_{}.pth\".format(random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "Agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std)\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "episode_n = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "    \n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len):\n",
        "        \n",
        "        # select action with policy\n",
        "        action = Agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # saving reward and is_terminals\n",
        "        Agent.buffer.rewards.append(reward)\n",
        "        Agent.buffer.dones.append(done)\n",
        "        \n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            Agent.update()\n",
        "\n",
        "\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(episode_n, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(episode_n, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "            \n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            Agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            \n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    episode_n += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**plotting learning curve**"
      ],
      "metadata": {
        "id": "TnTItu69XkDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "33dfMplHl-8W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "data= pd.read_csv('/content/Logs/PPO_RoboschoolAtlasForwardWalk-v1_log_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hWF8DXl_XaZ-",
        "outputId": "72ce2c0f-e077-4444-b2e2-b92d49e7ee06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-372dc56b-40a0-4366-99a6-dc2e2e854eb0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode</th>\n",
              "      <th>timestep</th>\n",
              "      <th>reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70</td>\n",
              "      <td>2000</td>\n",
              "      <td>42.1334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>141</td>\n",
              "      <td>4000</td>\n",
              "      <td>41.9877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>212</td>\n",
              "      <td>6000</td>\n",
              "      <td>45.6661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>283</td>\n",
              "      <td>8000</td>\n",
              "      <td>41.7731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>353</td>\n",
              "      <td>10000</td>\n",
              "      <td>45.9115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>30095</td>\n",
              "      <td>992000</td>\n",
              "      <td>112.9763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>30145</td>\n",
              "      <td>994000</td>\n",
              "      <td>108.3683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>30194</td>\n",
              "      <td>996000</td>\n",
              "      <td>111.4996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>30242</td>\n",
              "      <td>998000</td>\n",
              "      <td>114.3506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>30294</td>\n",
              "      <td>1000000</td>\n",
              "      <td>104.7072</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-372dc56b-40a0-4366-99a6-dc2e2e854eb0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-372dc56b-40a0-4366-99a6-dc2e2e854eb0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-372dc56b-40a0-4366-99a6-dc2e2e854eb0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     episode  timestep    reward\n",
              "0         70      2000   42.1334\n",
              "1        141      4000   41.9877\n",
              "2        212      6000   45.6661\n",
              "3        283      8000   41.7731\n",
              "4        353     10000   45.9115\n",
              "..       ...       ...       ...\n",
              "495    30095    992000  112.9763\n",
              "496    30145    994000  108.3683\n",
              "497    30194    996000  111.4996\n",
              "498    30242    998000  114.3506\n",
              "499    30294   1000000  104.7072\n",
              "\n",
              "[500 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes=list(data[\"episode\"])\n",
        "rewards=list(data[\"reward\"])"
      ],
      "metadata": {
        "id": "cxb3YVLrYid2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(episodes, rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "9HQT_ld5YrG0",
        "outputId": "907b0219-abd9-46f4-a413-0fb582718c08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5a32e85550>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NmBtN210oGYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "PROJET_DRL1_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
