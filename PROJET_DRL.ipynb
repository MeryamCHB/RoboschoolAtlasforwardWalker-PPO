{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4l1VL26bUWF",
        "outputId": "05b595fd-9dff-4426-ba44-9d12d4f0cc56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboschool==1.0.48 in /usr/local/lib/python3.7/dist-packages (1.0.48)\n",
            "Requirement already satisfied: gym==0.15.4 in /usr/local/lib/python3.7/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install roboschool==1.0.48 gym==0.15.4  #install compatible versions of roboschool and  gym \n",
        "\n",
        "!pip install box2d-py # 2d rigid body physics engine\n",
        "\n",
        "!pip install pybullet # for physics simulation and robotics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOG4FJMoxt2j"
      },
      "source": [
        "**Import libraries and set device to gpu**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lsGHSa40roi6"
      },
      "outputs": [],
      "source": [
        "import os  #provides functions for interacting with the operating system\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn          #pytorch neural networks package\n",
        "from torch.distributions import MultivariateNormal  #prob distibution for continious action space \n",
        "import numpy as np \n",
        "import gym           # a toolkit for developing and comparing reinforcement learning algorithms\n",
        "import roboschool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJCLSXEMOx2i",
        "outputId": "b68ade02-dda8-4747-9119-f83460a02aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "#Switch to GPU\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BTe0hVSjsEuh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self):\n",
        "        self.actions = []   # initialise actions\n",
        "        self.states = []    # initialise states\n",
        "        self.probs = []     # initialise probabilities\n",
        "        self.rewards = []   # initialise rewards\n",
        "        self.dones = []     # initialise dones \n",
        "    \n",
        "     #update memory  with the new state,action and logarithmic action probability \n",
        "    def store_memory(self, state, action, action_logprob):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(action_logprob)\n",
        "    \n",
        "    \n",
        "    #reset the memory\n",
        "    def clear_memory(self):\n",
        "        self.actions=[]\n",
        "        self.states=[]\n",
        "        self.probs=[]\n",
        "        self.rewards=[]\n",
        "        self.dones=[]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.action_dim = action_dim  #initilize with the action space dimention\n",
        "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor neural network\n",
        "        self.actor = nn.Sequential(\n",
        "                        nn.Linear(state_dim,64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64 ,64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, action_dim), \n",
        "                        nn.Tanh()   \n",
        "                    )\n",
        "\n",
        "\n",
        "        \n",
        "        # critic neural network\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)  #the critic will map state to its quality which is the Q value\n",
        "                    )\n",
        "    \n",
        "    def act(self, state):\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)   #normal prob distribution using diag covariance matrix\n",
        "        action = dist.sample()                   #sampling actions\n",
        "        action_logprob = dist.log_prob(action)   #log scale \n",
        "        return action.detach(), action_logprob.detach()\n",
        "\n",
        "    \n",
        "    def criticize(self, state, action):\n",
        "        action_mean = self.actor(state)\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(device)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action_probs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_probs, state_values, dist_entropy\n",
        "\n",
        "#PPO algorithm\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std_init=0.1):\n",
        "\n",
        "\n",
        "        self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma  #discount factor\n",
        "        self.eps_clip = eps_clip  #epsilon, clip parameter \n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = PPOMemory()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim,  action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim,  action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "    #select the adequate action according to probs\n",
        "    def select_action(self, state):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "        self.buffer.store_memory(state,action,action_logprob)\n",
        "\n",
        "\n",
        "        return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "            \n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        # rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_probs = torch.squeeze(torch.stack(self.buffer.probs, dim=0)).detach().to(device)\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            probs, state_values, dist_entropy = self.policy.criticize(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(probs - old_probs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()   \n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear_memory()\n",
        "    \n",
        "    #saving\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "    #loading\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "76WiiB8-Bqlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**environnement & algorithm Hyperparemeters** "
      ],
      "metadata": {
        "id": "ISWqLhJYBubl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"RoboschoolAtlasForwardWalk-v1\"\n",
        "max_ep_len = 1000                   # max timesteps in one episode\n",
        "max_training_timesteps = int(2000000)   # break training loop if timeteps > max_training_timesteps\n",
        "print_freq = max_ep_len * 10        # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2           # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = 10000             # save model frequency (in num timesteps)\n",
        "action_std = 0.1                    # starting std for action distribution (Multivariate Normal)\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 80               # update policy for K epochs in one PPO update The idea in PPO is that you want to reuse the batch many times to update the current policy.This means you repeat your training' k. epoch amount of times for the same batch of trajectories.\n",
        "eps_clip = 0.2          # clip parameter for PPO 2 \n",
        "gamma = 0.99            # discount factor\n",
        "lr_actor = 0.001       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n"
      ],
      "metadata": {
        "id": "4wX2e1QIBuDM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xCuXGQ5lJZw",
        "outputId": "d2ba7958-dab5-4421-f4a5-c3c0fd80cc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logging at : Logs/PPO_RoboschoolAtlasForwardWalk-v1_log_1.csv\n",
            "save checkpoint path : Checkpoints/RoboschoolAtlasForwardWalk-v1/checkpoint_0_0.pth\n",
            "state space dimension :  70\n",
            "action space dimension :  30\n",
            "Episode : 422 \t\t Timestep : 10000 \t\t Average Reward : 49.33\n",
            "model saved\n",
            "Episode : 848 \t\t Timestep : 20000 \t\t Average Reward : 49.85\n",
            "model saved\n",
            "Episode : 1271 \t\t Timestep : 30000 \t\t Average Reward : 51.04\n",
            "model saved\n",
            "Episode : 1692 \t\t Timestep : 40000 \t\t Average Reward : 51.01\n",
            "model saved\n",
            "Episode : 2109 \t\t Timestep : 50000 \t\t Average Reward : 52.33\n",
            "model saved\n",
            "Episode : 2529 \t\t Timestep : 60000 \t\t Average Reward : 53.01\n",
            "model saved\n",
            "Episode : 2940 \t\t Timestep : 70000 \t\t Average Reward : 55.69\n",
            "model saved\n",
            "Episode : 3347 \t\t Timestep : 80000 \t\t Average Reward : 56.85\n",
            "model saved\n",
            "Episode : 3759 \t\t Timestep : 90000 \t\t Average Reward : 57.08\n",
            "model saved\n",
            "Episode : 4167 \t\t Timestep : 100000 \t\t Average Reward : 58.23\n",
            "model saved\n",
            "Episode : 4572 \t\t Timestep : 110000 \t\t Average Reward : 58.98\n",
            "model saved\n",
            "Episode : 4974 \t\t Timestep : 120000 \t\t Average Reward : 61.14\n",
            "model saved\n",
            "Episode : 5371 \t\t Timestep : 130000 \t\t Average Reward : 63.07\n",
            "model saved\n",
            "Episode : 5766 \t\t Timestep : 140000 \t\t Average Reward : 64.65\n",
            "model saved\n",
            "Episode : 6157 \t\t Timestep : 150000 \t\t Average Reward : 66.1\n",
            "model saved\n",
            "Episode : 6545 \t\t Timestep : 160000 \t\t Average Reward : 68.08\n",
            "model saved\n",
            "Episode : 6936 \t\t Timestep : 170000 \t\t Average Reward : 67.84\n",
            "model saved\n",
            "Episode : 7319 \t\t Timestep : 180000 \t\t Average Reward : 70.7\n",
            "model saved\n",
            "Episode : 7699 \t\t Timestep : 190000 \t\t Average Reward : 70.99\n",
            "model saved\n",
            "Episode : 8074 \t\t Timestep : 200000 \t\t Average Reward : 72.8\n",
            "model saved\n",
            "Episode : 8447 \t\t Timestep : 210000 \t\t Average Reward : 74.47\n",
            "model saved\n",
            "Episode : 8818 \t\t Timestep : 220000 \t\t Average Reward : 75.38\n",
            "model saved\n",
            "Episode : 9186 \t\t Timestep : 230000 \t\t Average Reward : 76.33\n",
            "model saved\n",
            "Episode : 9542 \t\t Timestep : 240000 \t\t Average Reward : 78.93\n",
            "model saved\n",
            "Episode : 9901 \t\t Timestep : 250000 \t\t Average Reward : 79.05\n",
            "model saved\n",
            "Episode : 10255 \t\t Timestep : 260000 \t\t Average Reward : 80.83\n",
            "model saved\n",
            "Episode : 10605 \t\t Timestep : 270000 \t\t Average Reward : 82.57\n",
            "model saved\n",
            "Episode : 10953 \t\t Timestep : 280000 \t\t Average Reward : 82.47\n",
            "model saved\n",
            "Episode : 11298 \t\t Timestep : 290000 \t\t Average Reward : 83.42\n",
            "model saved\n",
            "Episode : 11640 \t\t Timestep : 300000 \t\t Average Reward : 85.45\n",
            "model saved\n",
            "Episode : 11974 \t\t Timestep : 310000 \t\t Average Reward : 86.41\n",
            "model saved\n",
            "Episode : 12307 \t\t Timestep : 320000 \t\t Average Reward : 87.46\n",
            "model saved\n",
            "Episode : 12635 \t\t Timestep : 330000 \t\t Average Reward : 89.17\n",
            "model saved\n",
            "Episode : 12966 \t\t Timestep : 340000 \t\t Average Reward : 88.31\n",
            "model saved\n",
            "Episode : 13292 \t\t Timestep : 350000 \t\t Average Reward : 89.17\n",
            "model saved\n",
            "Episode : 13614 \t\t Timestep : 360000 \t\t Average Reward : 89.56\n",
            "model saved\n",
            "Episode : 13934 \t\t Timestep : 370000 \t\t Average Reward : 91.69\n",
            "model saved\n",
            "Episode : 14252 \t\t Timestep : 380000 \t\t Average Reward : 91.88\n",
            "model saved\n",
            "Episode : 14567 \t\t Timestep : 390000 \t\t Average Reward : 92.53\n",
            "model saved\n",
            "Episode : 14873 \t\t Timestep : 400000 \t\t Average Reward : 94.86\n",
            "model saved\n",
            "Episode : 15183 \t\t Timestep : 410000 \t\t Average Reward : 93.29\n",
            "model saved\n",
            "Episode : 15488 \t\t Timestep : 420000 \t\t Average Reward : 95.45\n",
            "model saved\n",
            "Episode : 15794 \t\t Timestep : 430000 \t\t Average Reward : 94.97\n",
            "model saved\n",
            "Episode : 16095 \t\t Timestep : 440000 \t\t Average Reward : 96.85\n",
            "model saved\n",
            "Episode : 16397 \t\t Timestep : 450000 \t\t Average Reward : 95.54\n",
            "model saved\n",
            "Episode : 16697 \t\t Timestep : 460000 \t\t Average Reward : 97.24\n",
            "model saved\n",
            "Episode : 16995 \t\t Timestep : 470000 \t\t Average Reward : 97.24\n",
            "model saved\n",
            "Episode : 17284 \t\t Timestep : 480000 \t\t Average Reward : 99.74\n",
            "model saved\n",
            "Episode : 17573 \t\t Timestep : 490000 \t\t Average Reward : 100.72\n",
            "model saved\n",
            "Episode : 17862 \t\t Timestep : 500000 \t\t Average Reward : 98.79\n",
            "model saved\n",
            "Episode : 18146 \t\t Timestep : 510000 \t\t Average Reward : 102.38\n",
            "model saved\n",
            "Episode : 18418 \t\t Timestep : 520000 \t\t Average Reward : 107.07\n",
            "model saved\n",
            "Episode : 18692 \t\t Timestep : 530000 \t\t Average Reward : 104.87\n",
            "model saved\n",
            "Episode : 18961 \t\t Timestep : 540000 \t\t Average Reward : 104.7\n",
            "model saved\n",
            "Episode : 19233 \t\t Timestep : 550000 \t\t Average Reward : 105.32\n",
            "model saved\n",
            "Episode : 19501 \t\t Timestep : 560000 \t\t Average Reward : 107.42\n",
            "model saved\n",
            "Episode : 19764 \t\t Timestep : 570000 \t\t Average Reward : 108.25\n",
            "model saved\n",
            "Episode : 20019 \t\t Timestep : 580000 \t\t Average Reward : 111.98\n",
            "model saved\n",
            "Episode : 20276 \t\t Timestep : 590000 \t\t Average Reward : 110.04\n",
            "model saved\n",
            "Episode : 20526 \t\t Timestep : 600000 \t\t Average Reward : 115.12\n",
            "model saved\n",
            "Episode : 20772 \t\t Timestep : 610000 \t\t Average Reward : 115.36\n",
            "model saved\n",
            "Episode : 21013 \t\t Timestep : 620000 \t\t Average Reward : 119.01\n",
            "model saved\n",
            "Episode : 21260 \t\t Timestep : 630000 \t\t Average Reward : 115.05\n",
            "model saved\n",
            "Episode : 21496 \t\t Timestep : 640000 \t\t Average Reward : 119.75\n",
            "model saved\n",
            "Episode : 21730 \t\t Timestep : 650000 \t\t Average Reward : 121.05\n",
            "model saved\n",
            "Episode : 21966 \t\t Timestep : 660000 \t\t Average Reward : 119.72\n",
            "model saved\n",
            "Episode : 22202 \t\t Timestep : 670000 \t\t Average Reward : 121.06\n",
            "model saved\n",
            "Episode : 22431 \t\t Timestep : 680000 \t\t Average Reward : 125.56\n",
            "model saved\n",
            "Episode : 22661 \t\t Timestep : 690000 \t\t Average Reward : 124.1\n",
            "model saved\n",
            "Episode : 22885 \t\t Timestep : 700000 \t\t Average Reward : 128.58\n",
            "model saved\n",
            "Episode : 23109 \t\t Timestep : 710000 \t\t Average Reward : 128.57\n",
            "model saved\n",
            "Episode : 23328 \t\t Timestep : 720000 \t\t Average Reward : 129.29\n",
            "model saved\n",
            "Episode : 23546 \t\t Timestep : 730000 \t\t Average Reward : 130.85\n",
            "model saved\n",
            "Episode : 23766 \t\t Timestep : 740000 \t\t Average Reward : 129.62\n",
            "model saved\n",
            "Episode : 23982 \t\t Timestep : 750000 \t\t Average Reward : 130.42\n",
            "model saved\n",
            "Episode : 24198 \t\t Timestep : 760000 \t\t Average Reward : 132.24\n",
            "model saved\n",
            "Episode : 24408 \t\t Timestep : 770000 \t\t Average Reward : 135.46\n",
            "model saved\n",
            "Episode : 24610 \t\t Timestep : 780000 \t\t Average Reward : 139.77\n",
            "model saved\n",
            "Episode : 24819 \t\t Timestep : 790000 \t\t Average Reward : 134.11\n",
            "model saved\n",
            "Episode : 25023 \t\t Timestep : 800000 \t\t Average Reward : 135.99\n",
            "model saved\n",
            "Episode : 25230 \t\t Timestep : 810000 \t\t Average Reward : 136.09\n",
            "model saved\n",
            "Episode : 25435 \t\t Timestep : 820000 \t\t Average Reward : 137.64\n",
            "model saved\n",
            "Episode : 25642 \t\t Timestep : 830000 \t\t Average Reward : 136.32\n",
            "model saved\n",
            "Episode : 25851 \t\t Timestep : 840000 \t\t Average Reward : 133.88\n",
            "model saved\n",
            "Episode : 26053 \t\t Timestep : 850000 \t\t Average Reward : 140.91\n",
            "model saved\n",
            "Episode : 26251 \t\t Timestep : 860000 \t\t Average Reward : 145.38\n",
            "model saved\n",
            "Episode : 26449 \t\t Timestep : 870000 \t\t Average Reward : 144.36\n",
            "model saved\n",
            "Episode : 26654 \t\t Timestep : 880000 \t\t Average Reward : 137.93\n",
            "model saved\n",
            "Episode : 26856 \t\t Timestep : 890000 \t\t Average Reward : 141.81\n",
            "model saved\n",
            "Episode : 27054 \t\t Timestep : 900000 \t\t Average Reward : 144.3\n",
            "model saved\n",
            "Episode : 27252 \t\t Timestep : 910000 \t\t Average Reward : 145.56\n",
            "model saved\n",
            "Episode : 27454 \t\t Timestep : 920000 \t\t Average Reward : 142.22\n",
            "model saved\n",
            "Episode : 27652 \t\t Timestep : 930000 \t\t Average Reward : 144.69\n",
            "model saved\n",
            "Episode : 27849 \t\t Timestep : 940000 \t\t Average Reward : 146.23\n",
            "model saved\n",
            "Episode : 28050 \t\t Timestep : 950000 \t\t Average Reward : 142.94\n",
            "model saved\n",
            "Episode : 28243 \t\t Timestep : 960000 \t\t Average Reward : 148.98\n",
            "model saved\n",
            "Episode : 28442 \t\t Timestep : 970000 \t\t Average Reward : 143.98\n",
            "model saved\n",
            "Episode : 28637 \t\t Timestep : 980000 \t\t Average Reward : 150.22\n",
            "model saved\n",
            "Episode : 28833 \t\t Timestep : 990000 \t\t Average Reward : 149.63\n",
            "model saved\n",
            "Episode : 29022 \t\t Timestep : 1000000 \t\t Average Reward : 156.08\n",
            "model saved\n",
            "Episode : 29217 \t\t Timestep : 1010000 \t\t Average Reward : 148.26\n",
            "model saved\n",
            "Episode : 29413 \t\t Timestep : 1020000 \t\t Average Reward : 146.28\n",
            "model saved\n",
            "Episode : 29603 \t\t Timestep : 1030000 \t\t Average Reward : 153.08\n",
            "model saved\n",
            "Episode : 29796 \t\t Timestep : 1040000 \t\t Average Reward : 150.56\n",
            "model saved\n",
            "Episode : 29989 \t\t Timestep : 1050000 \t\t Average Reward : 150.34\n",
            "model saved\n",
            "Episode : 30176 \t\t Timestep : 1060000 \t\t Average Reward : 156.72\n",
            "model saved\n",
            "Episode : 30368 \t\t Timestep : 1070000 \t\t Average Reward : 152.37\n",
            "model saved\n",
            "Episode : 30565 \t\t Timestep : 1080000 \t\t Average Reward : 148.05\n",
            "model saved\n",
            "Episode : 30755 \t\t Timestep : 1090000 \t\t Average Reward : 153.28\n",
            "model saved\n",
            "Episode : 30944 \t\t Timestep : 1100000 \t\t Average Reward : 155.64\n",
            "model saved\n",
            "Episode : 31136 \t\t Timestep : 1110000 \t\t Average Reward : 152.48\n",
            "model saved\n",
            "Episode : 31330 \t\t Timestep : 1120000 \t\t Average Reward : 150.13\n",
            "model saved\n",
            "Episode : 31519 \t\t Timestep : 1130000 \t\t Average Reward : 156.69\n",
            "model saved\n",
            "Episode : 31705 \t\t Timestep : 1140000 \t\t Average Reward : 157.51\n",
            "model saved\n",
            "Episode : 31889 \t\t Timestep : 1150000 \t\t Average Reward : 160.6\n",
            "model saved\n",
            "Episode : 32077 \t\t Timestep : 1160000 \t\t Average Reward : 156.24\n",
            "model saved\n",
            "Episode : 32256 \t\t Timestep : 1170000 \t\t Average Reward : 162.66\n",
            "model saved\n",
            "Episode : 32447 \t\t Timestep : 1180000 \t\t Average Reward : 150.67\n",
            "model saved\n",
            "Episode : 32631 \t\t Timestep : 1190000 \t\t Average Reward : 157.83\n",
            "model saved\n",
            "Episode : 32815 \t\t Timestep : 1200000 \t\t Average Reward : 158.6\n",
            "model saved\n",
            "Episode : 32997 \t\t Timestep : 1210000 \t\t Average Reward : 160.19\n",
            "model saved\n",
            "Episode : 33177 \t\t Timestep : 1220000 \t\t Average Reward : 163.96\n",
            "model saved\n",
            "Episode : 33358 \t\t Timestep : 1230000 \t\t Average Reward : 163.58\n",
            "model saved\n",
            "Episode : 33539 \t\t Timestep : 1240000 \t\t Average Reward : 162.03\n",
            "model saved\n",
            "Episode : 33719 \t\t Timestep : 1250000 \t\t Average Reward : 161.86\n",
            "model saved\n",
            "Episode : 33895 \t\t Timestep : 1260000 \t\t Average Reward : 167.01\n",
            "model saved\n",
            "Episode : 33895 \t\t Timestep : 1260000 \t\t Average Reward : 167.01\n",
            "model saved\n",
            "Episode : 34076 \t\t Timestep : 1270000 \t\t Average Reward : 160.28\n",
            "model saved\n",
            "Episode : 34076 \t\t Timestep : 1270000 \t\t Average Reward : 160.28\n",
            "model saved\n",
            "Episode : 34258 \t\t Timestep : 1280000 \t\t Average Reward : 159.33\n",
            "model saved\n",
            "Episode : 34258 \t\t Timestep : 1280000 \t\t Average Reward : 159.33\n",
            "model saved\n",
            "Episode : 34435 \t\t Timestep : 1290000 \t\t Average Reward : 165.26\n",
            "model saved\n",
            "Episode : 34435 \t\t Timestep : 1290000 \t\t Average Reward : 165.26\n",
            "model saved\n",
            "Episode : 34614 \t\t Timestep : 1300000 \t\t Average Reward : 162.65\n",
            "model saved\n",
            "Episode : 34614 \t\t Timestep : 1300000 \t\t Average Reward : 162.65\n",
            "model saved\n",
            "Episode : 34788 \t\t Timestep : 1310000 \t\t Average Reward : 166.01\n",
            "model saved\n",
            "Episode : 34788 \t\t Timestep : 1310000 \t\t Average Reward : 166.01\n",
            "model saved\n",
            "Episode : 34962 \t\t Timestep : 1320000 \t\t Average Reward : 166.65\n",
            "model saved\n",
            "Episode : 34962 \t\t Timestep : 1320000 \t\t Average Reward : 166.65\n",
            "model saved\n",
            "Episode : 35138 \t\t Timestep : 1330000 \t\t Average Reward : 163.8\n",
            "model saved\n",
            "Episode : 35138 \t\t Timestep : 1330000 \t\t Average Reward : 163.8\n",
            "model saved\n",
            "Episode : 35309 \t\t Timestep : 1340000 \t\t Average Reward : 171.94\n",
            "model saved\n",
            "Episode : 35309 \t\t Timestep : 1340000 \t\t Average Reward : 171.94\n",
            "model saved\n",
            "Episode : 35483 \t\t Timestep : 1350000 \t\t Average Reward : 167.58\n",
            "model saved\n",
            "Episode : 35483 \t\t Timestep : 1350000 \t\t Average Reward : 167.58\n",
            "model saved\n",
            "Episode : 35654 \t\t Timestep : 1360000 \t\t Average Reward : 168.41\n",
            "model saved\n",
            "Episode : 35654 \t\t Timestep : 1360000 \t\t Average Reward : 168.41\n",
            "model saved\n",
            "Episode : 35826 \t\t Timestep : 1370000 \t\t Average Reward : 169.53\n",
            "model saved\n",
            "Episode : 35826 \t\t Timestep : 1370000 \t\t Average Reward : 169.53\n",
            "model saved\n",
            "Episode : 36002 \t\t Timestep : 1380000 \t\t Average Reward : 163.41\n",
            "model saved\n",
            "Episode : 36002 \t\t Timestep : 1380000 \t\t Average Reward : 163.41\n",
            "model saved\n",
            "Episode : 36174 \t\t Timestep : 1390000 \t\t Average Reward : 167.79\n",
            "model saved\n",
            "Episode : 36174 \t\t Timestep : 1390000 \t\t Average Reward : 167.79\n",
            "model saved\n",
            "Episode : 36344 \t\t Timestep : 1400000 \t\t Average Reward : 169.23\n",
            "model saved\n",
            "Episode : 36344 \t\t Timestep : 1400000 \t\t Average Reward : 169.23\n",
            "model saved\n",
            "Episode : 36514 \t\t Timestep : 1410000 \t\t Average Reward : 171.37\n",
            "model saved\n",
            "Episode : 36514 \t\t Timestep : 1410000 \t\t Average Reward : 171.37\n",
            "model saved\n",
            "Episode : 36684 \t\t Timestep : 1420000 \t\t Average Reward : 169.35\n",
            "model saved\n",
            "Episode : 36684 \t\t Timestep : 1420000 \t\t Average Reward : 169.35\n",
            "model saved\n",
            "Episode : 36853 \t\t Timestep : 1430000 \t\t Average Reward : 172.24\n",
            "model saved\n",
            "Episode : 36853 \t\t Timestep : 1430000 \t\t Average Reward : 172.24\n",
            "model saved\n",
            "Episode : 37020 \t\t Timestep : 1440000 \t\t Average Reward : 173.03\n",
            "model saved\n",
            "Episode : 37020 \t\t Timestep : 1440000 \t\t Average Reward : 173.03\n",
            "model saved\n",
            "Episode : 37188 \t\t Timestep : 1450000 \t\t Average Reward : 174.39\n",
            "model saved\n",
            "Episode : 37188 \t\t Timestep : 1450000 \t\t Average Reward : 174.39\n",
            "model saved\n",
            "Episode : 37359 \t\t Timestep : 1460000 \t\t Average Reward : 170.4\n",
            "model saved\n",
            "Episode : 37359 \t\t Timestep : 1460000 \t\t Average Reward : 170.4\n",
            "model saved\n",
            "Episode : 37525 \t\t Timestep : 1470000 \t\t Average Reward : 175.23\n",
            "model saved\n",
            "Episode : 37525 \t\t Timestep : 1470000 \t\t Average Reward : 175.23\n",
            "model saved\n",
            "Episode : 37693 \t\t Timestep : 1480000 \t\t Average Reward : 172.26\n",
            "model saved\n",
            "Episode : 37693 \t\t Timestep : 1480000 \t\t Average Reward : 172.26\n",
            "model saved\n",
            "Episode : 37859 \t\t Timestep : 1490000 \t\t Average Reward : 177.07\n",
            "model saved\n",
            "Episode : 37859 \t\t Timestep : 1490000 \t\t Average Reward : 177.07\n",
            "model saved\n",
            "Episode : 38029 \t\t Timestep : 1500000 \t\t Average Reward : 169.2\n",
            "model saved\n",
            "Episode : 38029 \t\t Timestep : 1500000 \t\t Average Reward : 169.2\n",
            "model saved\n",
            "Episode : 38195 \t\t Timestep : 1510000 \t\t Average Reward : 174.45\n",
            "model saved\n",
            "Episode : 38195 \t\t Timestep : 1510000 \t\t Average Reward : 174.45\n",
            "model saved\n",
            "Episode : 38357 \t\t Timestep : 1520000 \t\t Average Reward : 179.27\n",
            "model saved\n",
            "Episode : 38357 \t\t Timestep : 1520000 \t\t Average Reward : 179.27\n",
            "model saved\n",
            "Episode : 38521 \t\t Timestep : 1530000 \t\t Average Reward : 176.36\n",
            "model saved\n",
            "Episode : 38521 \t\t Timestep : 1530000 \t\t Average Reward : 176.36\n",
            "model saved\n",
            "Episode : 38681 \t\t Timestep : 1540000 \t\t Average Reward : 183.22\n",
            "model saved\n",
            "Episode : 38681 \t\t Timestep : 1540000 \t\t Average Reward : 183.22\n",
            "model saved\n",
            "Episode : 38842 \t\t Timestep : 1550000 \t\t Average Reward : 181.14\n",
            "model saved\n",
            "Episode : 38842 \t\t Timestep : 1550000 \t\t Average Reward : 181.14\n",
            "model saved\n",
            "Episode : 39004 \t\t Timestep : 1560000 \t\t Average Reward : 181.16\n",
            "model saved\n",
            "Episode : 39004 \t\t Timestep : 1560000 \t\t Average Reward : 181.16\n",
            "model saved\n",
            "Episode : 39163 \t\t Timestep : 1570000 \t\t Average Reward : 183.99\n",
            "model saved\n",
            "Episode : 39163 \t\t Timestep : 1570000 \t\t Average Reward : 183.99\n",
            "model saved\n",
            "Episode : 39324 \t\t Timestep : 1580000 \t\t Average Reward : 181.97\n",
            "model saved\n",
            "Episode : 39324 \t\t Timestep : 1580000 \t\t Average Reward : 181.97\n",
            "model saved\n",
            "Episode : 39488 \t\t Timestep : 1590000 \t\t Average Reward : 179.77\n",
            "model saved\n",
            "Episode : 39488 \t\t Timestep : 1590000 \t\t Average Reward : 179.77\n",
            "model saved\n",
            "Episode : 39647 \t\t Timestep : 1600000 \t\t Average Reward : 183.55\n",
            "model saved\n",
            "Episode : 39647 \t\t Timestep : 1600000 \t\t Average Reward : 183.55\n",
            "model saved\n",
            "Episode : 39807 \t\t Timestep : 1610000 \t\t Average Reward : 180.02\n",
            "model saved\n",
            "Episode : 39807 \t\t Timestep : 1610000 \t\t Average Reward : 180.02\n",
            "model saved\n",
            "Episode : 39964 \t\t Timestep : 1620000 \t\t Average Reward : 187.04\n",
            "model saved\n",
            "Episode : 39964 \t\t Timestep : 1620000 \t\t Average Reward : 187.04\n",
            "model saved\n",
            "Episode : 40125 \t\t Timestep : 1630000 \t\t Average Reward : 181.34\n",
            "model saved\n",
            "Episode : 40125 \t\t Timestep : 1630000 \t\t Average Reward : 181.34\n",
            "model saved\n",
            "Episode : 40283 \t\t Timestep : 1640000 \t\t Average Reward : 185.46\n",
            "model saved\n",
            "Episode : 40283 \t\t Timestep : 1640000 \t\t Average Reward : 185.46\n",
            "model saved\n"
          ]
        }
      ],
      "source": [
        "# Training \n",
        "env = gym.make(env_name) # create the environnement \n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "# action space dimension\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "# logging \n",
        "\n",
        "\n",
        "# assure that log files for multiple runs are NOT overwritten we will have a log file for each run \n",
        "\n",
        "log_dir = \"Logs\"\n",
        "if not os.path.exists(log_dir):# if no log directory\n",
        "      os.makedirs(log_dir) # create it \n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files) #number of existing log files \n",
        "\n",
        "\n",
        "\n",
        "#### create new log file for each run \n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "\n",
        "\n",
        "# checkpointing \n",
        "\n",
        "run_num_pretrained = 0 # change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"Checkpoints\" # initialise the checkpoint directory \n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory) # if no checkpoints dir create one \n",
        "#create new chpt file for each run \n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"checkpoint_{}_{}.pth\".format(random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "# training procedure\n",
        "\n",
        "Agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip,  action_std) # initialize a PPO agent\n",
        "\n",
        "# precise logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "time_step = 0 # to keep track of number of timesteps \n",
        "episode_n = 0 # to keep track of number of episodes \n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps: # while training is not done \n",
        "    #new episode\n",
        "    state = env.reset() # reset the environnement, It returns an initial observation\n",
        "    current_ep_reward = 0 # reset reward for each episode \n",
        "\n",
        "    for t in range(1, max_ep_len): # \n",
        "        \n",
        "        # select action with policy\n",
        "        action = Agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action) # take a step and returns four parameters, namely observation, reward, done and info.\n",
        "\n",
        "        Agent.buffer.rewards.append(reward)# saving reward \n",
        "        Agent.buffer.dones.append(done)# saving is_terminals\n",
        "        \n",
        "        time_step +=1 # the agent took a step  \n",
        "        current_ep_reward += reward # accumulate the reward for each episode\n",
        "\n",
        "        # update PPO agent each update_timestep\n",
        "        if time_step % update_timestep == 0:\n",
        "            Agent.update()\n",
        "\n",
        "\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(episode_n, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(episode_n, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "            \n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            Agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            \n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    episode_n += 1 # increment number of episodes \n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**plotting learning curve**"
      ],
      "metadata": {
        "id": "TnTItu69XkDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33dfMplHl-8W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # data manipulation\n",
        "import numpy as np #for mathematical operations on arrays\n",
        "import matplotlib.pyplot as plt #for curves plotting \n",
        "data= pd.read_csv('/content/Logs/PPO_RoboschoolAtlasForwardWalk-v1_log_1.csv') # reaading the log file "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "hWF8DXl_XaZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes=list(data[\"episode\"])\n",
        "rewards=list(data[\"reward\"])"
      ],
      "metadata": {
        "id": "cxb3YVLrYid2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(episodes, rewards) # plotting the average reward/episodes \n"
      ],
      "metadata": {
        "id": "9HQT_ld5YrG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NmBtN210oGYp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "PROJET_DRL1_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
